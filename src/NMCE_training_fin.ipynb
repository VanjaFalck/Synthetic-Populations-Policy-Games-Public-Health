{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f936507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from functions.data_cleaning import DataClean as dc\n",
    "from functions.figure_plotting import FigurePlot as fp\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rcParams\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "# For NMCE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from functions.utilities import *\n",
    "from functions.metrics import *\n",
    "from vae.vae_torch import VAE\n",
    "from gan.wgan_torch import Critic, Generator\n",
    "from nmce.manifold_clustering import MaximalCodingRateReduction, Z_loss, chunk_avg, Gumble_Softmax, get_data, MLP_net\n",
    "# Layout\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0226ea8",
   "metadata": {},
   "source": [
    "# Finland NMCE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "#Avant Garde palette\n",
    "CB91_Brown = \"#7d5a1b\"\n",
    "CB91_Brown_Gray = \"#816b51\"\n",
    "CB91_Green = \"#294013\"\n",
    "CB91_Light_Green = \"#d8ffc4\"\n",
    "CB91_Red = \"#84290d\"\n",
    "\n",
    "# Out of palette\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Yellow = \"#dfc077\"\n",
    "\n",
    "Green_Grad = [\"#d8ffc4\", \"#bbdfa7\", \"#9ebf89\", \"#81a06c\", \"#63804e\", \"#466031\", \n",
    "              \"#294013\", \"#24380f\", \"#1e310b\", \"#192908\", \"#132204\", \"#0e1a00\"]\n",
    "Red_Grad = [\"#fff1e3\", \"#efc6b0\", \"#df9b7d\", \"#cf6f4a\", \"#bf4417\", \"#ab3b14\",\n",
    "            \"#983210\", \"#84290d\", \"#70200a\", \"#5c1707\", \"#490e03\", \"#350500\"]\n",
    "Brown_Grad = [\"#fffac3\", \"#efda9d\", \"#dfc077\", \"#cfa550\",  \"#bf8b2a\", \"#a97a24\", \n",
    "              \"#93681e\", \"#7d5718\", \"#684512\", \"#52340c\", \"#3c2206\", \"#261100\"]\n",
    "\n",
    "color_list = [CB91_Brown, CB91_Green, CB91_Light_Green, CB91_Red,\n",
    "              CB91_Purple, CB91_Violet,  CB91_Yellow, CB91_Brown_Gray]\n",
    "\n",
    "#A list of hex colours running between blue and purple\n",
    "CB91_Grad_BP = ['#2cbdfe', '#2fb9fc', '#33b4fa', '#36b0f8',\n",
    "                '#3aacf6', '#3da8f4', '#41a3f2', '#449ff0',\n",
    "                '#489bee', '#4b97ec', '#4f92ea', '#528ee8',\n",
    "                '#568ae6', '#5986e4', '#5c81e2', '#607de0',\n",
    "                '#6379de', '#6775dc', '#6a70da', '#6e6cd8',\n",
    "                '#7168d7', '#7564d5', '#785fd3', '#7c5bd1',\n",
    "                '#7f57cf', '#8353cd', '#864ecb', '#894ac9',\n",
    "                '#8d46c7', '#9042c5', '#943dc3', '#9739c1',\n",
    "                '#9b35bf', '#9e31bd', '#a22cbb', '#a528b9',\n",
    "                '#a924b7', '#ac20b5', '#b01bb3', '#b317b1']\n",
    "\n",
    "sns.set(font=\"Verdana\",\n",
    "        rc={\n",
    "        \"axes.axisbelow\": False,\n",
    "        \"axes.edgecolor\": \"lightgrey\",\n",
    "        \"axes.facecolor\": \"None\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.labelcolor\": \"dimgrey\",\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"figure.facecolor\": \"white\",\n",
    "        \"lines.solid_capstyle\": \"round\",\n",
    "        \"patch.edgecolor\": \"w\",\n",
    "        \"patch.force_edgecolor\": True,\n",
    "        \"text.color\": \"dimgrey\",\n",
    "        \"xtick.bottom\": False,\n",
    "        \"xtick.color\": \"dimgrey\",\n",
    "        \"xtick.direction\": \"out\",\n",
    "        \"xtick.top\": False,\n",
    "        \"ytick.color\": \"dimgrey\",\n",
    "        \"ytick.direction\": \"out\",\n",
    "        \"ytick.left\": False,\n",
    "        \"ytick.right\": False})\n",
    " \n",
    "custom_colors = color_list\n",
    "sns.set_palette(sns.color_palette(color_list, 8))\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"font.size\":16,\n",
    "                                \"axes.titlesize\":20,\n",
    "                                \"axes.labelsize\":18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fd8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = True\n",
    "causal_model = False\n",
    "vae_model_number = 2  # 0=15, 1=30, 2=50, 3=100\n",
    "gan_model_number = 3\n",
    "\n",
    "def get_generated(torch_model, dim_batch, lat_feature):\n",
    "    noise = torch.rand(dim_batch, lat_feature)\n",
    "    with torch.no_grad():\n",
    "        s_data = torch_model(noise)\n",
    "    s_data = s_data.detach()\n",
    "    return s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e879a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_configuration_file = \"configurations/torch_vae.yaml\"\n",
    "gan_model_configuration_file = \"configurations/torch_gan.yaml\"\n",
    "nmce_model_configuration_file = \"configurations/torch_nmce.yaml\"\n",
    "preprocessing_configuration_file = \"configurations/categorical_preprocessing_configuration.yaml\"\n",
    "plotting_configuration_file = \"configurations/print.yaml\"\n",
    "draw = fp(plotting_configuration_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c349274",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = \"data/original_preprocessed.csv\"\n",
    "begin = time.time()\n",
    "data = dc(datafile=data_filename,\n",
    "          prepared=True,\n",
    "          configuration_file=preprocessing_configuration_file\n",
    "          )\n",
    "duration = time.time() - begin\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vae_model_configuration_file, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "vae_cfg = cfg[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gan_model_configuration_file, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "gan_cfg = cfg[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(nmce_model_configuration_file, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "nmce_cfg = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4baa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "if causal_model:\n",
    "    df = data.get_data_causal()\n",
    "else:\n",
    "    df = data.get_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333441b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = nmce_cfg[\"globals\"][\"model_name\"][0]\n",
    "model_type = nmce_cfg[\"globals\"][\"model_type\"]  # \"categorical\"\n",
    "fig_dpi = draw.cfg[\"fig_dpi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33fe008",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data.get_data() # data to run in deep generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee50b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dimension = df.shape[1]\n",
    "vae_latent_dimension = int(vae_cfg[\"latent_dimensions\"][vae_model_number])\n",
    "gan_latent_dimension = int(gan_cfg[\"latent_dimensions\"][gan_model_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vae latent: {}\\tWgan latent: {}\".format(vae_latent_dimension, gan_latent_dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28add032",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_learning_rate = float(gan_cfg[\"learning_rate\"][0])\n",
    "beta_1 = float(gan_cfg[\"beta1\"])\n",
    "beta_2 = float(gan_cfg[\"beta2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if causal_model:\n",
    "    feature_dimension_base = data.get_data().shape[1]\n",
    "    critic = Critic(feature_dimension_base, output_dim=1)\n",
    "    generator = Generator(feature_dimension_base, gan_latent_dimension)\n",
    "else:\n",
    "    critic = Critic(feature_dimension, output_dim=1)\n",
    "    generator = Generator(feature_dimension, gan_latent_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_critic = optim.Adam(critic.parameters(), lr=gan_learning_rate, betas=(beta_1, beta_2))  # GAN + WGAN-GP\n",
    "opt_generator = optim.Adam(generator.parameters(), lr=gan_learning_rate, betas=(beta_1, beta_2))  # GAN + WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec481a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_learning_rate = float(vae_cfg[\"learning_rate\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f526cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae = VAE(feature_dimension, vae_latent_dimension)\n",
    "optimiser = optim.RMSprop(model_vae.parameters(), lr=vae_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if causal_model:\n",
    "    vae_dict = torch.load(vae_cfg[\"model_dict_file_causal_\" + str(vae_latent_dimension)])\n",
    "    opt_dict = torch.load(vae_cfg[\"opt_dict_file_causal_\" + str(vae_latent_dimension)])\n",
    "else:\n",
    "    vae_dict = torch.load(vae_cfg[\"model_dict_file_\" + str(vae_latent_dimension)])\n",
    "    opt_dict = torch.load(vae_cfg[\"opt_dict_file_\" + str(vae_latent_dimension)])\n",
    "\n",
    "model_vae.load_state_dict(vae_dict)\n",
    "optimiser.load_state_dict(opt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gan_cfg[\"generator_dict_file_100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ccd38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator.load_state_dict(torch.load(gan_cfg[\"generator_dict_file_100\"]))\n",
    "opt_generator.load_state_dict(torch.load(gan_cfg[\"opt_gen_dict_file_100\"]))\n",
    "critic.load_state_dict(torch.load(gan_cfg[\"critic_dict_file_100\"]))\n",
    "opt_critic.load_state_dict(torch.load(gan_cfg[\"opt_critic_dict_file_100\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model_vae.encoder\n",
    "decoder = model_vae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transfer learning can be used by building on a freezed model \n",
    "or taking the outputs from another model as input to a new\n",
    "\n",
    "Helper functions to freeze a model:\n",
    "set requrie_grads = False on parameters\n",
    "\"\"\"\n",
    "\n",
    "def freeze_model(model):\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = False\n",
    "        print(\"_\"*20)\n",
    "        print(f\"name: {name}\")\n",
    "        print(\"values: \")\n",
    "        print(para)\n",
    "def unfreeze_model(model):\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = True\n",
    "        print(\"_\"*20)\n",
    "        print(f\"name: {name}\")\n",
    "        print(\"values: \")\n",
    "        print(para)\n",
    "\n",
    "def show_model_parameters(model):\n",
    "    for name, para in model.named_parameters():\n",
    "        print(\"_\"*20)\n",
    "        print(f\"name: {name}\")\n",
    "        print(\"values: \")\n",
    "        print(para)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab5223",
   "metadata": {},
   "source": [
    "# Create Synthetic Population\n",
    "\n",
    "1. Decide sample size (default = same as origininal data)\n",
    "2. Create tf_synthetic which is a tensorflow EagerTensor object representing data\n",
    "3. Convert EagerTensor object to pandas dataframe\n",
    "4. Compare original and synthetic population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12cf11c",
   "metadata": {},
   "source": [
    "# Classified Resampling of Synthetic Population\n",
    "\n",
    "The difference in clustering patterns between the original data and the synthetic populations (either gan or vae), shows than these neural networks do not capture statistical underlying patterns despite the good reproduction of a complete synthetic population against the original data. This is a known weakness of neural networks, that can be counteracted with methods that capture these structures. Neural manifold clustering and embedding is suggested as one such method.\n",
    "\n",
    "1. Synthetic populations are generated using the pretrained VAE or WGAN models\n",
    "2. The trained NMCE is used to cluster the original data and sets of synthetic data to demonstrate differences or similarities in clustering\n",
    "3. For each cluster in the original data, cluster data are extracted and a new batch of synthetic data based on these cluster data created with the pretrained VAE or WGAN. \n",
    "4. Then the combined original and synthetic single-cluster data are used to train a second VAE, that now specialice on a single cluster.\n",
    "5. The complete workflow from original data, through a first batch of synthetic data and a second cluster specific VAE or WGAN results in a model that can output a desired number of clusters. We first test this for the complete population, and next to create a profiled synthetic population for regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch_synthetic from pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f95c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.get_data()\n",
    "df_causal_o = data.get_data_causal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc04302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_causal_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_synthetic_gan = get_generated(generator, df.shape[0], gan_latent_dimension)\n",
    "torch_synthetic_vae = get_generated(decoder, df.shape[0], vae_latent_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99159796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = data.get_synthetic(torch_synthetic_gan, df.columns)\n",
    "df_sv = data.get_synthetic(torch_synthetic_vae, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc7d4d",
   "metadata": {},
   "source": [
    "# Plot Univariate Marginals WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1935082",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = pd.concat([df.mean(), df_s.mean()], axis=1)\n",
    "wgan_column = \"WGAN-\" + str(gan_latent_dimension)\n",
    "combine.columns = [\"Original\", wgan_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hots\n",
    "draw.plot_compare(data=combine,\n",
    "                  title=\"compare\",\n",
    "                  model_type=model_type,\n",
    "                  model=\"WGAN\",\n",
    "                  model_name=model_name,\n",
    "                  #save=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4486a",
   "metadata": {},
   "source": [
    "# Plot Univariate Marginals VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_vae = combine_data(df, df_sv, [\"Original\", \"VAE-\" + str(vae_latent_dimension)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8341105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hots\n",
    "draw.plot_compare(data=combine_vae,\n",
    "                  title=\"compare\",\n",
    "                  model_type=model_type,\n",
    "                  model=\"VAE\",\n",
    "                  model_name=model_name,\n",
    "                  #save=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77cb33a9",
   "metadata": {},
   "source": [
    "df_o_cat = data.get_data_recategorised()\n",
    "df_s_cat = data.get_synthetic_recategorised(torch_synthetic, df_o.columns)\n",
    "\n",
    "for col in df_o_cat.columns:\n",
    "    if df_o_cat[col].value_counts().max() > 11000:\n",
    "        scale_max=df_o_cat[col].value_counts().max()\n",
    "    else:\n",
    "        scale_max=None\n",
    "    if (data.min_values[col] < 1):\n",
    "        n_bins = int(data.max_bins[col] + 1)\n",
    "        scale_min = 0\n",
    "    else:\n",
    "        scale_min = 1\n",
    "        n_bins = int(data.max_bins[col])\n",
    "    draw.plot_two(df_original=df_o_cat, \n",
    "                  df_synthetic=df_s_cat,\n",
    "                  title=col,\n",
    "                  hue_value=\"isFemale\",\n",
    "                  n_bins=n_bins,\n",
    "                  model_type=model_type,\n",
    "                  model=\"vae\",\n",
    "                  model_name=model_name,\n",
    "                  scale_max=scale_max,\n",
    "                  scale_min=scale_min,\n",
    "                  #save=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1b16c80",
   "metadata": {},
   "source": [
    "for col in df_o_cat.columns:\n",
    "    if df_o_cat[col].value_counts().max() > 11000:\n",
    "        scale_max=int(df_o_cat[col].value_counts().max())\n",
    "    else:\n",
    "        scale_max=11000\n",
    "    if (data.min_values[col] < 1):\n",
    "        n_bins = int(data.max_bins[col]) + 1\n",
    "        scale_min = -1\n",
    "    else:\n",
    "        scale_min = 0\n",
    "        n_bins = int(data.max_bins[col])\n",
    "    draw.plot_dist(data=df_o_cat,\n",
    "                   title=col,\n",
    "                   bins=n_bins + 1,\n",
    "                   model_type=model_type,\n",
    "                   model=\"vae_original\",\n",
    "                   model_name=model_name,\n",
    "                   scale_min=scale_min,\n",
    "                   kde=False,\n",
    "                   #save=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbd63d42",
   "metadata": {},
   "source": [
    "for col in df_s_cat.columns:\n",
    "    if df_s_cat[col].value_counts().max() > 11000:\n",
    "        scale_max=int(df_s_cat[col].value_counts().max())\n",
    "    else:\n",
    "        scale_max=11000\n",
    "    if (data.min_values[col] < 1):\n",
    "        n_bins = int(data.max_bins[col]) + 1\n",
    "        scale_min = -1\n",
    "    else:\n",
    "        scale_min = 0\n",
    "        n_bins = int(data.max_bins[col])\n",
    "    draw.plot_dist(data=df_s_cat,\n",
    "                   title=col,\n",
    "                   bins=n_bins + 1,\n",
    "                   scale_min=scale_min,\n",
    "                   #scale_max=scale_max,\n",
    "                   model_type=model_type,\n",
    "                   model=\"vae_synthetic\",\n",
    "                   model_name=model_name,\n",
    "                   kde=False,\n",
    "                   #save=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811cb1e9",
   "metadata": {},
   "source": [
    "# Check for similar records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = data.get_data()\n",
    "duplicate_records = len(df_o)-len(df_o.drop_duplicates())\n",
    "print (duplicate_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o[df_o.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = data.get_synthetic(torch_synthetic_gan, df_o.columns)\n",
    "duplicate_records = len(df_s)-len(df_s.drop_duplicates())\n",
    "print (duplicate_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb464018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s[df_s.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23d9dd",
   "metadata": {},
   "source": [
    "# Classification with NMCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out x data record in order to get the batch size in an integer divide size\n",
    "x_data = torch.tensor(df.iloc[:-1,:].values, dtype=torch.float32)\n",
    "x_data = x_data + 0.00001  # To avoid 0\n",
    "runs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_loss = []\n",
    "d_loss = []\n",
    "z_sim_list = []\n",
    "n_steps = 1000\n",
    "print_every = 300\n",
    "bs = 1929 # Gives 10 iterations\n",
    "n_chunks = 2  # MUST be hardcoded to same value in .py file!!!!\n",
    "#task variables\n",
    "amb_dim = feature_dimension  # input dim\n",
    "lat_dim = 150  # neurons at each layer\n",
    "z_dim = 100\n",
    "n_clusters = 20\n",
    "lambda_ = 40  # er 4000 for synthetic helix (ser ut til å spille mindre rolle)\n",
    "G_Softmax = Gumble_Softmax(0.2, straight_through=False)\n",
    "criterion = MaximalCodingRateReduction(eps=0.01, gamma=1.0)\n",
    "criterion_z = Z_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b464c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_new = False\n",
    "\n",
    "if train_new:\n",
    "    # Create new instance of net\n",
    "    net = MLP_net(amb_dim, lat_dim, z_dim, n_clusters)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9,0.99), weight_decay=0.00001)    \n",
    "else:\n",
    "    # Using the last saved to keep training:\n",
    "    net = torch.jit.load('models/nmce/fin/late_nmce_model_RUN_15_1929_230_125.06042_21.24433_0.69456scripted.pt') \n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9,0.99), weight_decay=0.00001) \n",
    "    \"\"\"\n",
    "    \n",
    "Latest 20 runs with random noise (x + noise)/2\n",
    "Start from last model\n",
    "New 20 runs with random noise (x + x + noise)/2\n",
    "Crashed at run 15 ---> start again from this\n",
    "\n",
    "\n",
    "\n",
    "USE BS size --->\n",
    "\n",
    "NORWAY: n = 24270 (all)\n",
    "ba = batch size\n",
    "1618 give 15 steps\n",
    "2427 give 10 steps\n",
    "\n",
    "FINLAND: n = 19290 (skip one record)\n",
    "bs = batch size \n",
    "643 give 30 steps, \n",
    "1286 give 15 steps, (first runs using this)\n",
    "1929 give 10 steps,\n",
    "3858 give 5 step\n",
    "\"\"\"\n",
    "begin = time.time()\n",
    "for run in range(runs):\n",
    "    for i in range(n_steps):\n",
    "        loader = iter(DataLoader(dataset=x_data, batch_size=bs, shuffle=True))\n",
    "        # Run one batch and update grads\n",
    "        for j in range(len(loader)):\n",
    "            x = next(loader)\n",
    "            # Create augmented data from vae-model\n",
    "            noise = torch.rand(bs, x.shape[1])\n",
    "            aug_latent = encoder((x + x + x + noise) / 4)\n",
    "            xn = decoder(aug_latent)\n",
    "            # xn = torch.tensor(np.array(xn), dtype=torch.float32)\n",
    "            xt = torch.cat((xn, x), dim=0).float()\n",
    "            z, logits = net(xt)\n",
    "            loss_z, z_sim = criterion_z(z)\n",
    "            z_sim = z_sim.mean()\n",
    "            prob = G_Softmax(logits)\n",
    "            z, prob = chunk_avg(z, n_chunks=n_chunks, normalize=True), chunk_avg(prob, n_chunks=n_chunks)\n",
    "            loss, loss_list= criterion(z,prob,num_classes=n_clusters)\n",
    "            loss += lambda_ * loss_z\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Save the loss at the end of run of all batches \n",
    "            if j % (len(loader) - 1) == 0:\n",
    "                c_loss.append(loss_list[0])\n",
    "                d_loss.append(loss_list[1])\n",
    "                z_sim_list.append(z_sim.item())\n",
    "        if i % print_every == 0:\n",
    "            print('{} steps, loss c {:.5f}, loss d {:.5f}, z sim {:.5f}'.format(i+1,loss_list[0],loss_list[1],z_sim.item()))\n",
    "    print(\"Duration for run {} is {}\".format(run, duration))\n",
    "    print(\"C_loss: \", c_loss[-1])\n",
    "    print(\"D_loss: \", d_loss[-1])\n",
    "    print(\"Z_sim: \", z_sim_list[-1])\n",
    "    if run % 1 == 0 or run == runs:\n",
    "        model_scripted = torch.jit.script(net) # Export to TorchScript\n",
    "        model_scripted.save('models/nmce/fin/late_nmce_model_RUN_{}_{}_{}_{:.5f}_{:.5f}_{:.5f}scripted.pt'.format(run, bs, feature_dimension, c_loss[-1], d_loss[-1], z_sim_list[-1])) # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(net) # Export to TorchScript\n",
    "model_scripted.save('models/nmce/fin/late_nmce_model_RUN_{}_{}_{}_{:.5f}_{:.5f}_{:.5f}scripted.pt'.format(run, bs, feature_dimension, c_loss[-1], d_loss[-1], z_sim_list[-1])) # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ee3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = time.time() - begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_curves = True\n",
    "plt.plot(c_loss[12::2], label=\"c-loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('C-Loss')\n",
    "plt.legend()\n",
    "if save_curves:\n",
    "    plt.savefig(\"figures/nmce-experimental-fin/Next_Normal_C_loss_\" + str(n_steps * runs) +\n",
    "                \"_clusters_\" + str(n_clusters) +\n",
    "                \"_batch_\" + str(bs) +\n",
    "                \"_features_\" + str(feature_dimension) +\n",
    "                \"_lat_\" + str(vae_latent_dimension) + \n",
    "                \".png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a93266",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_loss[12::2], label=\"d-loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('D-Loss')\n",
    "plt.legend()\n",
    "if save_curves:\n",
    "    plt.savefig(\"figures/nmce-experimental-fin/Next_Normal_D_loss_\" + str(n_steps * runs) +\n",
    "                \"_clusters_\" + str(n_clusters) +\n",
    "                \"_batch_\" + str(bs) +\n",
    "                \"_features_\" + str(feature_dimension) +\n",
    "                \"_lat_\" + str(vae_latent_dimension) + \n",
    "                \".png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z_sim_list[12::2], label=\"z-sim\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Z-Sim')\n",
    "plt.legend()\n",
    "if save_curves:\n",
    "    plt.savefig(\"figures/nmce-experimental-fin/Next_Normal_Z_sim_\" + str(n_steps * runs) +\n",
    "                \"_clusters_\" + str(n_clusters) +\n",
    "                \"_batch_\" + str(bs) +\n",
    "                \"_features_\" + str(feature_dimension) +\n",
    "                \"_lat_\" + str(vae_latent_dimension) + \n",
    "                \".png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ad55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_new = data.get_synthetic(torch_synthetic_vae, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1S = torch.tensor(df_s_new.values, dtype=torch.float32) # synthetic\n",
    "x2O = torch.tensor(df_o.values, dtype=torch.float32) # original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_1 = get_predictions(net, df_s_new)\n",
    "df_preds_2 = get_predictions(net, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b31fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_1_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e85ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_1 = pd.DataFrame(preds_1.numpy()) # Synthetic data\n",
    "df_preds_2 = pd.DataFrame(preds_2.numpy()) # Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_cluster = df_preds_2.value_counts().index.tolist()\n",
    "highest_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest = highest_cluster[0][0]\n",
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_highest = data.get_data_recategorised()\n",
    "df_o_highest[\"cluster\"] = df_preds_2\n",
    "df_s_highest = data.get_synthetic_recategorised(torch_synthetic_gan, df.columns)\n",
    "df_s_highest[\"cluster\"] = df_preds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af649f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_highest = df_o_highest.loc[df_o_highest[\"cluster\"] == highest].copy()\n",
    "df_s_highest = df_s_highest.loc[df_s_highest[\"cluster\"] == highest].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.categorical:\n",
    "    if df_o_highest[col].value_counts().max() > 11000:\n",
    "        scale_max=df_o_highest[col].value_counts().max()\n",
    "    else:\n",
    "        scale_max=None\n",
    "    if (data.min_values[col] < 1):\n",
    "        n_bins = int(data.max_bins[col] + 1)\n",
    "        scale_min = 0\n",
    "    else:\n",
    "        scale_min = 1\n",
    "        n_bins = int(data.max_bins[col])\n",
    "    draw.plot_two(df_original=df_o_highest, \n",
    "                  df_synthetic=df_s_highest,\n",
    "                  title=col,\n",
    "                  hue_value=\"isFemale\",\n",
    "                  n_bins=n_bins,\n",
    "                  model_type=model_type,\n",
    "                  model=\"vae_highest_cluster_only \",\n",
    "                  model_name=model_name,\n",
    "                  scale_max=scale_max,\n",
    "                  scale_min=scale_min,\n",
    "                  #save=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_highest_hot = data.get_data()\n",
    "df_o_highest_hot[\"cluster\"] = df_preds_2\n",
    "df_s_highest_hot = data.get_synthetic(torch_synthetic_gan, df.columns)\n",
    "df_s_highest_hot[\"cluster\"] = df_preds_1\n",
    "\n",
    "df_o_highest_hot = df_o_highest_hot.loc[df_o_highest_hot[\"cluster\"] == highest].copy()\n",
    "df_s_highest_hot = df_s_highest_hot.loc[df_s_highest_hot[\"cluster\"] == highest].copy()\n",
    "\n",
    "\n",
    "original_highest_hot = (df_o_highest_hot.iloc[:,:-1].sum(axis=0) / df_o_highest_hot.shape[0])\n",
    "synthetic_highest_hot = (df_s_highest_hot.iloc[:,:-1].sum(axis=0) / df_s_highest_hot.shape[0])\n",
    "\n",
    "combine_highest_hot = pd.concat([original_highest_hot, synthetic_highest_hot], axis=1)\n",
    "combine_highest_hot.columns = [\"Original\", \"WGAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86725c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_highest_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ddfb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select the cluster with greatest number of examples and compare variables between\n",
    "original and syntetic data.\n",
    "\n",
    "\"\"\"\n",
    "draw.plot_compare(data=combine_highest_hot,\n",
    "                  title=\"compare\",\n",
    "                  model_type=model_type,\n",
    "                  model=\"VAE_highest_cluster\",\n",
    "                  model_name=model_name,\n",
    "                  #save=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05764c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_preds = pd.concat([df_preds_2.value_counts()/df_preds_2.shape[0], df_preds_1.value_counts()/df_preds_1.shape[0]], axis=1)\n",
    "combine_preds.columns = [\"Original\", \"Synthetic\"]\n",
    "combine_preds = combine_preds.replace(np.nan, 0)\n",
    "\n",
    "print(\"combine_preds shape: \", combine_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67195703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare frequencies on found clusters\n",
    "\"\"\"\n",
    "draw.plot_compare(data=combine_preds,\n",
    "                  title=\"compare_all\",\n",
    "                  model_type=model_type,\n",
    "                  model=\"augmented_clusters\",\n",
    "                  model_name=model_name,\n",
    "                  #save=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de22033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_cluster = df_o.copy()\n",
    "df_original_cluster[\"cluster\"] = df_preds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic_cluster = df_s_new.copy()\n",
    "df_synthetic_cluster[\"cluster\"] = df_preds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df_original_cluster, \n",
    "           x=\"cluster\", y=\"getHelp\", \n",
    "           col=\"hasIncome\", \n",
    "           hue=\"isFemale\",\n",
    "           ci=95,\n",
    "           seed=42,\n",
    "           x_ci=\"ci\",\n",
    "           fit_reg=True,\n",
    "           height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_13_o = df_original_cluster.copy()\n",
    "orig_13_s = df_synthetic_cluster.copy()\n",
    "orig_13_original = orig_13_o.loc[orig_13_o[\"cluster\"] == 2].copy()\n",
    "orig_13_synthetic = orig_13_s.loc[orig_13_s[\"cluster\"] == 2].copy()\n",
    "# Create a combo of original and synthetic data with the same predicted category\n",
    "# Trying to \"recreate\" this category in the vae\n",
    "orig_13 = pd.concat([orig_13_original, orig_13_synthetic], axis=0)\n",
    "orig_13 = orig_13.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33389299",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_13 = pd.concat([orig_13_original.iloc[:,:-1].mean(), orig_13_synthetic.iloc[:,:-1].mean()], axis=1)\n",
    "combine_13.columns = [\"SecondClusterOriginal\", \"SecondClusterSynthetic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c263804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select the cluster with second greatest number of examples and compare variables between\n",
    "original and syntetic data.\n",
    "\n",
    "\"\"\"\n",
    "draw.plot_compare(data=combine_13,\n",
    "                  title=\"compare\",\n",
    "                  model_type=model_type,\n",
    "                  model=\"VAE_second_highest_cluster\",\n",
    "                  model_name=model_name,\n",
    "                  #save=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e508d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_13.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.from_numpy(orig_13.values).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of \"same category\" augmented data and test it with the classifier\n",
    "# orig_13.values should be a tensor\n",
    "synth_13_lat = encoder(torch.as_tensor(orig_13.values.astype(np.float32)))\n",
    "synth_13_tf = decoder(synth_13_lat)\n",
    "synth_13 = np.array(synth_13_tf.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1S13 = torch.tensor(synth_13, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, logits = net(x1S13)\n",
    "    pred_s_13 = logits.max(dim=1)[1]\n",
    "x1S13 = x1S13.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daadfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_13_df = pd.DataFrame(pred_s_13.numpy())\n",
    "synth_13_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_13_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mu, log_var): # std = sigma ** 2 log_var = log(sigma ** 2)\n",
    "    loss_kl = - 0.5 * torch.sum(1 + log_var - torch.exp(log_var) - mu ** 2)\n",
    "    return loss_kl\n",
    "\n",
    "loss_fn = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9709622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae.trainable = False\n",
    "decoder.trainable = False\n",
    "encoder.trainable = False\n",
    "number_epochs = 40\n",
    "# Train a new vae on small-size classes derived from this set-up\n",
    "model_13 = VAE(x_data.shape[1], vae_latent_dimension)\n",
    "optimiser_13 = optim.RMSprop(model_13.parameters(), lr=vae_learning_rate)\n",
    "model_13.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = torch.tensor(orig_13.values, dtype=torch.float32)\n",
    "batch_size = 128\n",
    "beta_vae = 0.5\n",
    "loader = DataLoader(torch_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train VAE model to produce the second largest cluster in original data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "collect_loss = []\n",
    "for epoch in range(number_epochs):\n",
    "    for batch_idx, (real) in enumerate(loader):\n",
    "        batch_size = real.shape[0]\n",
    "        replica, z_mean, z_sigma = model_vae(real)\n",
    "        reconstruction_loss = loss_fn(replica, real)\n",
    "        kl = beta_vae * (kl_loss(z_mean, z_sigma) / real.shape[1])\n",
    "        loss = reconstruction_loss + kl\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        if batch_idx % 20 == 0 and batch_idx > 0:\n",
    "            print(f\"Epoch [{epoch} / {number_epochs}] \\ \"\n",
    "                  f\"KL Loss: {kl:4f}, Rep Loss: {reconstruction_loss:.4f}\")\n",
    "            collect_loss.append((kl, reconstruction_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_13 = model_13.encoder\n",
    "decoder_13 = model_13.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, para in encoder_13.named_parameters():\n",
    "    print(\"_\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    print(\"values: \")\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264bbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = torch.randn(df.shape[0], vae_latent_dimension)\n",
    "torch_s = model_vae.decoder(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x_enc = encoder(torch.from_numpy(orig_13.values.astype(np.float32)))\n",
    "x_dec = decoder(x_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_synthetic_c = get_generated(decoder_13, orig_13.shape[0], vae_latent_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c45558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic_c = data.get_synthetic(tf_synthetic_c, orig_13.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = torch.tensor(df_synthetic_c.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z, logits = net(cat_data)\n",
    "    pred_cat = logits.max(dim=1)[1]\n",
    "cat_data = cat_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_cat = pd.DataFrame(pred_cat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fca037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic_c[\"cluster\"] = pred_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic population of cluster 6\n",
    "df_double = df_synthetic_c.loc[df_synthetic_c[\"cluster\"] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_double = torch.tensor(df_double.iloc[:,:-1].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93613d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z, logits = net(cat_double)\n",
    "    pred_double = logits.max(dim=1)[1]\n",
    "cat_double = cat_double.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_double = pd.DataFrame(pred_double.numpy())\n",
    "df_13_double.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8926a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the torch model (Saved trained net model)\n",
    "nmce_model = torch.jit.load('models/nmce/nor/nmce_model_RUN_19_1618_157_23.49887_22.89522_0.97810scripted.pt')\n",
    "nmce_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce822fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic_c = data.get_synthetic(tf_synthetic_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_nmce_data = torch.tensor(df_synthetic_c.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e91d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z, logits = nmce_model(cat_nmce_data)\n",
    "    pred_nmce_cat = logits.max(dim=1)[1]\n",
    "cat_nmce_data = cat_nmce_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33daff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_cat_nmce = pd.DataFrame(pred_nmce_cat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_cat_nmce.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b6dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a56ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499561e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc13bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
